<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Research &#8211; Dr Jian Chen</title>
	<atom:link href="http://localhost/wordpress/category/research/feed/" rel="self" type="application/rss+xml" />
	<link>http://jianch.github.io</link>
	<description></description>
	<lastBuildDate>Wed, 10 Feb 2021 03:52:13 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.1</generator>

<image>
	<url>http://jianch.github.io/wp-content/uploads/2021/02/cropped-Frued_Cartoon-32x32.jpg</url>
	<title>Research &#8211; Dr Jian Chen</title>
	<link>http://jianch.github.io</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Embed Youtube video in the Inquisit Web in Windows and Mac</title>
		<link>http://jianch.github.io/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/</link>
					<comments>http://jianch.github.io/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Mon, 06 Apr 2020 03:51:40 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Inquisit]]></category>
		<category><![CDATA[Mac]]></category>
		<category><![CDATA[OnlineVideo]]></category>
		<category><![CDATA[Windows]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=141</guid>

					<description><![CDATA[It is necessary to use online video source when you want to present long video clips in an Inquisit Web script. This is because the Inquisit Web server only provides 60MB space for you, although we all pay them a big amount of money, such a small storage space can only host a few short&#8230; <a class="more-link" href="http://jianch.github.io/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/">Continue reading <span class="screen-reader-text">Embed Youtube video in the Inquisit Web in Windows and Mac</span></a>]]></description>
										<content:encoded><![CDATA[
<p>It is necessary to use online video source when you want to present long video clips in an Inquisit Web script. This is because the Inquisit Web server only provides 60MB space for you, although we all pay them a big amount of money, such a small storage space can only host a few short and low resolution videos.</p>



<p>Thankfully, Inquisit is able to handle html page, which is great. Theoretically, you only need to embed an iframe element in the Inquisit script and it should be able to fetch video from Youtube or other sources.&nbsp;</p>



<p>You will need an embeded video link like this, it can be obtained from Youtube Share.</p>



<figure class="wp-block-embed is-type-rich is-provider-embed-handler wp-block-embed-embed-handler"><div class="wp-block-embed__wrapper">
https://youtube.com/watch?v=LW2oswdWs4Q%3Frel%3D0%26controls%3D0%26showinfo%3D0%26autoplay%3D1
</div></figure>



<p>Then you need to add a html element in the Inquisit, it looks like this:</p>



<pre class="wp-block-code"><code>&lt;html demo>
/ items = videos
/ select = 1
/ size = (100%,100%)
/ showborders = false
/ showscrollbars = false
&lt;/html></code></pre>



<p>However, this method only works fine in MacOS, it shows blank when you run the Inquisit script in a Windows computer. This is a known issue and there is no official solution yet.&nbsp;</p>



<p>————————————<br><strong>Q:&nbsp;</strong><br>I’ve used html to embed videos in my experiment that I have uploaded on youtube.<br>The embedded links when directly entered into the browser (firefox, internet explorer) work fine. Within inquisit it also works fine on a mac, but I’m encountering problems on windows (on some windows laptops it only shows a black box rather than the video). Also, there seem to be a difference between windows 10 and windows 7, since it did work on someone’s windows 7 laptop.<br>Is there a difference for html on mac vs windows?</p>



<p><strong>A:&nbsp;</strong><br>Yes, there are differences between OSX and Windows in how HTML is handled. Essentially, under Windows, Inquisit embeds Internet Explorer (because it is reliably available on all Windows systems) to render HTML. When run in embedded mode like this, however, Internet Explorer enforces various restrictions that may keep embedded / interactive content such as videos from working. The way a given system is set up and what security settings the user or organization applied to Internet Explorer installations also play a role under some circumstances, i.e. content may work on one system, but not on a different one with different settings / restrictions applied.<br><strong>I’m afraid there isn’t a really good or universal solution here.&nbsp;</strong>Ideally, you’d not embed Youtube videos, but instead you’d use standard \&lt;video&gt; elements in Inquisit to play the videos. For online use, you can set the \&lt;video&gt; elements’ /stream attributes to true, that way the won’t have to be downloaded in full before the experiment launches, they’ll be streamed instead at runtime.<br><em><a href="https://www.millisecond.com/forums/Topic25195.aspx" target="_blank" rel="noreferrer noopener">https://www.millisecond.com/forums/Topic25195.aspx</a></em><br>————————————</p>



<p>A possible workaround is to right click on the black screen and then choose a different encoding, such as “Western European (Windows)”. Then the online video will be loaded and displayed on the Inquisit. There seems to be an encoding issue in running Inquisit script in Windows.</p>



<figure class="wp-block-image size-large"><img loading="lazy" width="1024" height="473" src="http://jianch.github.io/wp-content/uploads/2021/02/rightClick-1024x473.png" alt="" class="wp-image-143" srcset="http://jianch.github.io/wp-content/uploads/2021/02/rightClick-1024x473.png 1024w, http://jianch.github.io/wp-content/uploads/2021/02/rightClick-300x139.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/rightClick-768x355.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/rightClick-1536x709.png 1536w, http://jianch.github.io/wp-content/uploads/2021/02/rightClick-2048x946.png 2048w, http://jianch.github.io/wp-content/uploads/2021/02/rightClick-850x393.png 850w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>So, is there a “<strong>really good or universal solution</strong>” for this problem? The answer is&nbsp;<strong>YES.</strong></p>



<p>My idea here is, if the Inquisit is able to handle html element, can we directly insert a DIY html page rather than let the Inquisit build a html page?&nbsp;</p>



<p>Here, I created a html page with some help from W3Schools and other resources:_(<a rel="noreferrer noopener" href="https://benmarshall.me/responsive-iframes/" target="_blank">https://benmarshall.me/responsive-iframes/</a>; <a rel="noreferrer noopener" href="https://www.w3schools.com/tags/att_body_bgcolor.asp" target="_blank">https://www.w3schools.com/tags/att_body_bgcolor.asp</a>; <a rel="noreferrer noopener" href="https://stackoverflow.com/questions/15844500/shrink-a-youtube-video-to-responsive-width" target="_blank">https://stackoverflow.com/questions/15844500/shrink-a-youtube-video-to-responsive-width</a>)_</p>



<pre class="wp-block-code"><code>&lt;!DOCTYPE html>
&lt;html>
&lt;head>
&lt;style>
.iframe-container {
 overflow: hidden;
 padding-top: 56.25%;
 position: relative;
}
.iframe-container iframe {
  border: 0;
  height: 100%;
  left: 0;
  position: absolute;
  top: 0;
  width: 100%;
}
/* 4x3 Aspect Ratio */
.iframe-container-4x3 {
 padding-top: 75%;
}
&lt;/style>
&lt;/head>

&lt;body bgcolor="black">

&lt;div class="iframe-container">
 &lt;iframe src="https://www.youtube.com/embed/LW2oswdWs4Q?~~start=10&amp;rel=0&amp;controls=0&amp;showinfo=0&amp;autoplay=1" allowfullscreen=1>&lt;/iframe>
&lt;/div>

&lt;/body>
&lt;/html></code></pre>



<p>You can create an empty html document and the copy these code into your document, or you can download a html file from&nbsp;<a href="https://jianchen.info/files/inquisitOnlineVideo/demo.html">here</a>.</p>



<p>When you have the DIY html file, let’s see an Inquisit <a href="https://jianchen.info/files/inquisitOnlineVideo/demo.iqx">demo</a>. This demo is extracted and modified from somewhere in the Inquisit forum. Nevertheless, it’s quite easy to make your own one.</p>



<pre class="wp-block-code"><code>&lt;html videoDemo>
/ items = ("demo.html")
/ size = (100%, 100%) 
/ erase = true(255, 255, 255)
/ showborders = false
/ showscrollbars = false
&lt;/html>

&lt;trial videoDemo>
/ stimulustimes = &#91;500=videoDemo]
/ trialduration = 50000
/ ontrialend= &#91;trial.videoDemo.resetstimulusframes();]
&lt;/trial>

&lt;block videoDemo>
/ trials = &#91;1=videoDemo]
&lt;/block>

&lt;expt>
/ blocks = &#91;1=videoDemo]
&lt;/expt></code></pre>



<p></p>



<p>And boom! It works! Now we can display Youtube video in Inquisit in a damn Windows pc!</p>



<p></p>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img loading="lazy" width="1024" height="583" src="http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-1024x583.png" alt="" data-id="144" data-full-url="http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot.png" data-link="http://jianch.github.io/?attachment_id=144" class="wp-image-144" srcset="http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-1024x583.png 1024w, http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-300x171.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-768x437.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-850x484.png 850w, http://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot.png 1425w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure></li><li class="blocks-gallery-item"><figure><img loading="lazy" width="930" height="713" src="http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot.png" alt="" data-id="145" data-full-url="http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot.png" data-link="http://jianch.github.io/?attachment_id=145" class="wp-image-145" srcset="http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot.png 930w, http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot-300x230.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot-768x589.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/iqxScreenshot-850x652.png 850w" sizes="(max-width: 930px) 100vw, 930px" /></figure></li></ul></figure>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Change the space between paragraphs in Qualtrics</title>
		<link>http://jianch.github.io/change-the-space-between-paragraphs-in-qualtrics/</link>
					<comments>http://jianch.github.io/change-the-space-between-paragraphs-in-qualtrics/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Tue, 12 Nov 2019 03:47:26 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Javascript]]></category>
		<category><![CDATA[Padding]]></category>
		<category><![CDATA[Qualtrics]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=139</guid>

					<description><![CDATA[The space between paragraphs in the Qualtrics might be too large, how to reduce the space?&#160; One solution is to add JS code in the “Question JavaScript” to change the padding size or margin size before and after one paragraph. Note, Qualtrics appears to set up some kind of minimum space between paragraphs so even&#8230; <a class="more-link" href="http://jianch.github.io/change-the-space-between-paragraphs-in-qualtrics/">Continue reading <span class="screen-reader-text">Change the space between paragraphs in Qualtrics</span></a>]]></description>
										<content:encoded><![CDATA[
<p>The space between paragraphs in the Qualtrics might be too large, how to reduce the space?&nbsp;</p>



<p>One solution is to add JS code in the “Question JavaScript” to change the padding size or margin size before and after one paragraph.</p>



<pre class="wp-block-code"><code>jQuery("#"+this.questionId).find('.QuestionText:first').css("padding-bottom", "0px");
jQuery("#"+this.questionId).find('.QuestionText:first').css("padding-top", "0px");</code></pre>



<p>Note, Qualtrics appears to set up some kind of minimum space between paragraphs so even if you change the padding value to 0, the space might still be large.</p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/change-the-space-between-paragraphs-in-qualtrics/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Plot Heatmap from Eye Tracking Data</title>
		<link>http://jianch.github.io/plot-heatmap-from-eye-tracking-data/</link>
					<comments>http://jianch.github.io/plot-heatmap-from-eye-tracking-data/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 07 Sep 2017 03:42:50 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Eyelink]]></category>
		<category><![CDATA[Eyetracking]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=130</guid>

					<description><![CDATA[There are many ways/toolbox to plot a heat map from eye tracking data. but I prefer DIY Here is the source code:]]></description>
										<content:encoded><![CDATA[
<p>There are many ways/toolbox to plot a heat map from eye tracking data. but I prefer DIY</p>



<hr class="wp-block-separator"/>



<p>Here is the source code:</p>



<pre class="wp-block-code"><code>%% Data Structure Explanation
% Explanation from Eyelink Programers Guide 3.0

%--------------------------------------------------------
% dataEyelink =
%
%       samples: &#91;401341x4 double]()
%     fixations: &#91;3965x6 double]()
%      saccades: &#91;3964x9 double]()
%        blinks: &#91;1720x3 double]()
%      triggers: &#91;1080x3 double]()

%--------------------------------------------------------
% dataEyelink.samples
%       column1         column2         column3     column4
%       timepoint       x               y           pupil size
%--------------------------------------------------------
% dataEyelink.fixations
%       column1         column2         column3     column4 column5 column6
%       tmepoint_start  timepoint_end   duration    x       y       avg pupil size
%--------------------------------------------------------
% dataEyelink.saccades
%       column1         column2         column3     column4 column5 column6
%       tmepoint_start  timepoint_end   duration    x_from  y_from  x_to
%       column7         column8         column9
%       y_to            amplitude       peak velocity
%                       in degrees      degr/sec
%--------------------------------------------------------
% dataEyelink.blinks
%       column1         column2         column3
%       tmepoint_start  timepoint_end   duration
%--------------------------------------------------------
% dataEyelink.triggers
%       column1         column2         column3
%       tmepoint        1=SYNCON        Trial(I also wrote trial number)
%                       0=SYNCOFF
%--------------------------------------------------------



clear;
load xxxxEyelink.mat

%% variables
gaussSigma = 0.05;
posX = round(dataEyelink.fixations(:,4));
posY = round(dataEyelink.fixations(:,5));
gazeDuration = dataEyelink.fixations(:,3) / max(dataEyelink.fixations(:,3)); % rescale to 0-1

%% generating data for heatmap
gazedata = &#91;posX/1024, posY/768](); % rescale to 0-1
gazedata = gazedata((gazedata(:, 1))\>0, :); % remove possible negative value...

%% make gaussians
figure;
&#91;X,Y]() = meshgrid(0:0.001:1, 0:0.001:1);
z = zeros(size(X,1),size(X,2));

for i = 1:length(gazedata)
z = z + gazeDuration(i) * exp(-( ((X - gazedata(i,1)).^2 ./ (2*gaussSigma^2)) + ((Y - gazedata(i,2)).^2 ./ (2*gaussSigma^2)) ) );
end

mesh(X,Y,z); % plot the heatmap
colorbar;
caxis(&#91;0,300]());
view(0,90);


print('heatmap', '-dtiff','-r300');</code></pre>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/plot-heatmap-from-eye-tracking-data/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Corrected Vision May Still Bias the Gabor Patch Detection</title>
		<link>http://jianch.github.io/corrected-vision-may-still-bias-the-gabor-patch-detection/</link>
					<comments>http://jianch.github.io/corrected-vision-may-still-bias-the-gabor-patch-detection/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Wed, 12 Jul 2017 03:40:14 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[astigmatism]]></category>
		<category><![CDATA[Gabor]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=128</guid>

					<description><![CDATA[As said in the title, participants with corrected vision (wearing glass or contact lens) may still have distorted vision due to whatever reason, especially in Gabor patch detection task. This question comes from my personal experience, I always feel that the vertical Gabor patch is brighter than horizontal ones, and is easier to detect. Below&#8230; <a class="more-link" href="http://jianch.github.io/corrected-vision-may-still-bias-the-gabor-patch-detection/">Continue reading <span class="screen-reader-text">Corrected Vision May Still Bias the Gabor Patch Detection</span></a>]]></description>
										<content:encoded><![CDATA[
<p>As said in the title, participants with corrected vision (wearing glass or contact lens) may still have distorted vision due to whatever reason, especially in Gabor patch detection task.</p>



<p>This question comes from my personal experience, I always feel that the vertical Gabor patch is brighter than horizontal ones, and is easier to detect. Below is my test data from 800 trials of a detection task (error bar represents 1 SE).</p>



<p>I gradually increased the Gabor patch contrast from 0 to 100%, I pressed a button when I saw something on the screen (grey background).</p>



<p><img src="https://jianchen.info/images/gabor/CJ04May17a.png" alt=""><br>Session 1</p>



<p><img src="https://jianchen.info/images/gabor/CJ04May17b.png" alt=""><br>Session 2</p>



<p>I am quite curious about why I feel so differently for these two kinds of Gabor patch, am I special? Do I have something wrong with my eye?</p>



<hr class="wp-block-separator"/>



<p>So I asked two colleague (they don’t wear glass regularly) to do the same task, and it turns out no difference between vertical and horizontal Gabor patches.</p>



<p><img src="https://jianchen.info/images/gabor/CV04May17a.png" alt=""><br>Colleague 1</p>



<p><img src="https://jianchen.info/images/gabor/JN06May17a.png" alt=""><br>Colleague 2</p>



<p>Then what’s going on underneath?</p>



<p>I doubt that although I have a glass to correct my vision, I may not have astigmatism corrected. So I asked a friend, who also wear a glass, to do the same task. As you can see, he has the same problem.</p>



<p><img src="https://jianchen.info/images/gabor/ZL28Jun17a.png" alt=""><br>Friend 1</p>



<p>Insofar, my guess of the explanation for such results is astigmatism. Me and my friend both have inappropriately corrected vision, which leads to the difference in detection task.</p>



<hr class="wp-block-separator"/>



<p>You may have a quick test to see if you have astigmatism. All the bars in below should have similar brightness.</p>



<p><img src="https://jianchen.info/images/gabor/EYE_astigmatism.gif" alt=""><br>Astigmatism Test</p>



<hr class="wp-block-separator"/>



<p>What’s the hint from this observation? Well, it may remind us to make sure that participants who claim a correctly corrected vision should be re-checked, especially in those vision tasks relying on Gabor patch detection.</p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/corrected-vision-may-still-bias-the-gabor-patch-detection/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Get real-time gaze positions from the eye tracke</title>
		<link>http://jianch.github.io/get-real-time-gaze-positions-from-the-eye-tracke/</link>
					<comments>http://jianch.github.io/get-real-time-gaze-positions-from-the-eye-tracke/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Tue, 30 May 2017 03:39:11 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Eyelink]]></category>
		<category><![CDATA[gaze]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=126</guid>

					<description><![CDATA[If I want to make sure the participants are fixating on the cross before starting each trial, how should I do? With the help of Eyetracker (Eyelink in this case), we can get the realtime gaze position and therefore we can monitor the gaze position to make sure it falls into a certain area before&#8230; <a class="more-link" href="http://jianch.github.io/get-real-time-gaze-positions-from-the-eye-tracke/">Continue reading <span class="screen-reader-text">Get real-time gaze positions from the eye tracke</span></a>]]></description>
										<content:encoded><![CDATA[
<p>If I want to make sure the participants are fixating on the cross before starting each trial, how should I do?</p>



<p>With the help of Eyetracker (Eyelink in this case), we can get the realtime gaze position and therefore we can monitor the gaze position to make sure it falls into a certain area before the participants start the trial.</p>



<hr class="wp-block-separator"/>



<pre class="wp-block-code"><code>% make sure participants are fixating on the central
while isEyeLink  % if Eyelink was connected
status = Eyelink('newfloatsampleavailable');  % check to see if everything is fine in Eyelink
if status /=0; % if all is fine
evt = Eyelink('newestfloatsample'); % get the newest float sample from Eyelink
realtime.x = evt.gx(1);  % get the x axis of gaze
realtime.y = evt.gy(1);  % get the y axis of gaze
end;

if abs(realtime.x-512)\&lt;=50&amp;abs(realtime.y-384)\&lt;=50  % if the gaze falls into  a 50\*50 pixel square around the cross
break; % jump out the loop and start the trial
end
end</code></pre>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/get-real-time-gaze-positions-from-the-eye-tracke/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Keyboard Calibration Using the Damn Laser</title>
		<link>http://jianch.github.io/keyboard-calibration-using-the-damn-laser/</link>
					<comments>http://jianch.github.io/keyboard-calibration-using-the-damn-laser/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 09 Mar 2017 03:37:30 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Bootstrapping]]></category>
		<category><![CDATA[Calibration]]></category>
		<category><![CDATA[CRT]]></category>
		<category><![CDATA[Keyboard]]></category>
		<category><![CDATA[Laser]]></category>
		<category><![CDATA[Linux]]></category>
		<category><![CDATA[Photodiode]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=124</guid>

					<description><![CDATA[Almost no one care about how precise/reliable the keyboard pressing is, here, we built a damn cool equipment to examine this question. Broadly speaking, the time precision is good, mean variance is less than 0.2 ms. Well, let me find the raw data first, since it’s been done for a long time. Environment: MATLAB with&#8230; <a class="more-link" href="http://jianch.github.io/keyboard-calibration-using-the-damn-laser/">Continue reading <span class="screen-reader-text">Keyboard Calibration Using the Damn Laser</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Almost no one care about how precise/reliable the keyboard pressing is, here, we built a damn cool equipment to examine this question. Broadly speaking, the time precision is good, mean variance is less than 0.2 ms.</p>



<p>Well, let me find the raw data first, since it’s been done for a long time.</p>



<hr class="wp-block-separator"/>



<p>Environment: MATLAB with Linux (Ubuntu 14.04)</p>



<p>Monitor: SONY Triniton at 120Hz</p>



<p>Keyboard: Mechanic number board</p>



<hr class="wp-block-separator"/>



<figure class="wp-block-image"><img src="https://jianchen.info/images/laser/illustration.png" alt=""/></figure>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/keyboard-calibration-using-the-damn-laser/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>EEG Processing Steps (based on EEGLAB and ERPLAB)</title>
		<link>http://jianch.github.io/eeg-processing-steps-based-on-eeglab-and-erplab/</link>
					<comments>http://jianch.github.io/eeg-processing-steps-based-on-eeglab-and-erplab/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Sat, 18 Feb 2017 03:35:37 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Biosemi]]></category>
		<category><![CDATA[EEG]]></category>
		<category><![CDATA[Linux]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=122</guid>

					<description><![CDATA[Here are the basic steps I used to process BioSemi EEG data. 1 load raw data 2 channel location 3 re-reference 4 filtering&#160;(sometimes the data is very huge and therefore the filtering will be very slow, so we can do the filtering later) 5 Create EEG event list (ERPLAB from now on) 6 Assign bins&#8230; <a class="more-link" href="http://jianch.github.io/eeg-processing-steps-based-on-eeglab-and-erplab/">Continue reading <span class="screen-reader-text">EEG Processing Steps (based on EEGLAB and ERPLAB)</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Here are the basic steps I used to process BioSemi EEG data.</p>



<p><strong>1 load raw data</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/809C496A-8039-4D2E-9B13-70B6B7721A10.png" alt=""/></figure>



<p><strong>2 channel location</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/030CC12C-9735-43A2-AD38-53EBF6240A46.png" alt=""/></figure>



<p><strong>3 re-reference</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/3135436B-A01A-4FED-9B9E-E426C7FA3CB4.png" alt=""/></figure>



<p><strong>4 filtering</strong>&nbsp;(sometimes the data is very huge and therefore the filtering will be very slow, so we can do the filtering later)</p>



<p><strong>5 Create EEG event list (ERPLAB from now on)</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/F18A0B75-3355-4EBE-B7CA-E62720858371.png" alt=""/></figure>



<p><strong>6 Assign bins (how to operate on bins? see&nbsp;<a href="https://github.com/lucklab/erplab/wiki/Assigning-Events-to-Bins-with-BINLISTER:-Tutorial" target="_blank" rel="noreferrer noopener">here</a>)</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/43ABAD39-3E41-4BA7-9607-34EA61FE7D74.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/8A8EC18C-86E4-4AB8-8242-1E6D23716321.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/4AC21F49-0134-4FE3-83E1-12B663C404DF.png" alt=""/></figure>



<p><strong>7 extract bin-based epoch</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/B1B02461-7F46-4AB1-8371-AF769E9F3E26.png" alt=""/></figure>



<p><strong>8 artifact rejection</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/40120CE4-073E-4CEF-A35D-98DDB2DDD429.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/9234B231-7612-41D2-9349-5682C3027FD8.png" alt=""/></figure>



<p><strong>-&gt; choose channel around the eye. -&gt; update and then reject</strong></p>



<p><strong>9 compute average erps</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/289AC38D-87CF-4234-9B90-D5C01D010D9A.png" alt=""/></figure>



<p><strong>10 filter if you don’t do it in step 4</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/CD5AD34D-B8EC-4825-B5A8-90EA5C31611D.png" alt=""/></figure>



<p><strong>11 bin operations if necessary&nbsp;</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/1E8272A4-A250-4471-945D-2D764439AE61.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/3FA48AA1-9212-4A49-A8B4-73CEFFB13B38.png" alt=""/></figure>



<p><strong>12 Plot ERPs</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/AB81310E-9BC5-422B-95FD-B3718883494D.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/F1F2CFBD-3018-4E77-B960-35EEFB103226.png" alt=""/></figure>



<p><strong>13 Save figures if you want</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/14A0D60A-40B2-4697-9F30-84312AFCEEBF.png" alt=""/></figure>



<p>Remember, these are basic steps to illustrate how to process the EEG data using EEGLAB and ERPLAB toolbox, you’re suggested not to strictly follow these steps.</p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/eeg-processing-steps-based-on-eeglab-and-erplab/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Load Eyelink data into R (in a very fast way)</title>
		<link>http://jianch.github.io/load-eyelink-data-into-r-in-a-very-fast-way/</link>
					<comments>http://jianch.github.io/load-eyelink-data-into-r-in-a-very-fast-way/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Tue, 17 Jan 2017 03:33:52 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[asc]]></category>
		<category><![CDATA[edf]]></category>
		<category><![CDATA[Eyelink]]></category>
		<category><![CDATA[Eyetracking]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=120</guid>

					<description><![CDATA[These scripts are adapted from Dr. Jason Forte’s work in our lab. R]]></description>
										<content:encoded><![CDATA[
<p>These scripts are adapted from Dr. Jason Forte’s work in our lab.</p>



<p></p>



<p>R</p>



<pre class="wp-block-code"><code>## eyelink data analysis 
setwd('/Users/JianChen/Dropbox/PhD/Exp1/')
filenameASC = 'xxxxxx.asc'
dataEyelink
# extract all numeric lines &amp;amp; replace blink (.) with 0.0\. Hack: sed
# ''s/\.//g'' gets rid of all decimal points. We correct for that in the line dataEyelink$samples$V2 &amp;amp; V3
# system command in this case = sed -n '/^\&#91;0-9]/p' PH12J17a.asc | sed 's/ \./0.0/g' | sed 's/\.//g' &amp;gt; tmp.txt
system(paste("sed -n '/^\&#91;0-9]/p'", filenameASC, " | sed 's/\\./0.0/g' "," | sed 's/\\.//g' &amp;gt; tmp.txt"))
# read samples into data structure
dataEyelink$samples &amp;lt;- read.table('tmp.txt')
# correct for above hack to resote pixel values....
dataEyelink$samples$V2 &amp;lt;- dataEyelink$samples$V2 / 10
dataEyelink$samples$V3 &amp;lt;- dataEyelink$samples$V3 / 10
# extract gaze event for "EFIX" &amp;amp; remove "EFIX R" (Here I tracked right eye)
system(paste("sed -n -e '/^EFIX/p' ", filenameASC, " | sed 's/EFIX R //g' &amp;gt; tmp.txt"))
# read fixations into data structure
dataEyelink$fixations &amp;lt;- read.table('tmp.txt')

# extract gaze event for "ESACC" &amp;amp; remove "ESACC R"
system(paste("sed -n -e '/^ESACC/p' ", filenameASC, " | sed 's/ESACC R //g' &amp;gt; tmp.txt"))
# read saccades into data structure
dataEyelink$saccades &amp;lt;- read.table('tmp.txt')

# extract gaze event for "EBLINK" &amp;amp; remove "EBLINK R"
system(paste("sed -n -e '/^EBLINK/p' ", filenameASC," | sed 's/EBLINK R //g' &amp;gt; tmp.txt"))
# read blinks into data structure
dataEyelink$blinks &amp;lt;- read.table('tmp.txt')

# extract trigger event for "SYNCTIMEON" &amp;amp; "SYNCTIMEOFF" convert "SYNCTIMEON" to 1 &amp;amp; "SYNCTIMEOFF" to 0 and remove MSG
system(paste("sed -n -e '/SYNCTIMEON/p' -e '/SYNCTIMEOFF/p' ", filenameASC, " | sed 's/SYNCTIMEON/1/g' | sed 's/SYNCTIMEOFF/0/g' | sed 's/MSG //g' &amp;gt; tmp.txt"))
# read triggers into data structure
dataEyelink$triggers &amp;lt;- read.table('tmp.txt')

system('rm tmp.txt')</code></pre>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/load-eyelink-data-into-r-in-a-very-fast-way/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Merge Behavioral Data into EEG</title>
		<link>http://jianch.github.io/merge-behavioral-data-into-eeg/</link>
					<comments>http://jianch.github.io/merge-behavioral-data-into-eeg/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Fri, 16 Dec 2016 03:05:54 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[EEG]]></category>
		<category><![CDATA[EEG.event]]></category>
		<category><![CDATA[ERP]]></category>
		<category><![CDATA[ERP.EVENTLIST.eventinfo]]></category>
		<category><![CDATA[ERPLAB]]></category>
		<category><![CDATA[Event]]></category>
		<category><![CDATA[EVENTLIST]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=94</guid>

					<description><![CDATA[An example code of how I merge behavioral data into EEG Code:]]></description>
										<content:encoded><![CDATA[
<p>An example code of how I merge behavioral data into EEG</p>



<p></p>



<p></p>



<figure class="wp-block-image size-large"><img loading="lazy" width="868" height="319" src="http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192101.png" alt="" class="wp-image-95" srcset="http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192101.png 868w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192101-300x110.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192101-768x282.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192101-850x312.png 850w" sizes="(max-width: 868px) 100vw, 868px" /></figure>



<figure class="wp-block-image size-large"><img loading="lazy" width="922" height="319" src="http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192033.png" alt="" class="wp-image-96" srcset="http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192033.png 922w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192033-300x104.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192033-768x266.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161213-192033-850x294.png 850w" sizes="(max-width: 922px) 100vw, 922px" /></figure>



<p></p>



<p>Code:</p>



<p></p>



<pre class="wp-block-code"><code>%% merge behavioral data into EEG

% Sometimes, it's hard to lively send behavioral data to EEG system, i.e.

% oral response. Here is a code example to merge behavioral result into EEG

% data.

% Author: Jian Chen

% saturn.jian.chen@gmail.com

% 13/Dec/2016

%+++++++++++++%

%% load EEG data first, finish 'channel location', 're-reference'

% get event data via 'event = EEG.event'

event = EEG.event;

event\_back = event;

%% load behavioral data

fid = fopen('YourResponse\_acc.txt');  % here is a vector with length of 540

acc = textscan(fid,'%d');

fclose(fid);

%% set the latency between target code and inserted code

time = 100;

%% creat response event

for i = 2 : 2 : length(event)

Nevent(i/2).bepoch    = event(i).bepoch;

Nevent(i/2).bini      = event(i).bini;

Nevent(i/2).binlabel  = event(i).binlabel;

Nevent(i/2).codelabel = event(i).codelabel;

Nevent(i/2).duration  = event(i).duration;

Nevent(i/2).enable    = event(i).enable;

Nevent(i/2).flag      = event(i).flag;

Nevent(i/2).item      = event(i).item + 1;

Nevent(i/2).latency   = event(i).latency + time;

Nevent(i/2).type      = acc{1}(i/2);

end

%% add new events to original event list

for i = 1:length(Nevent)

event(2*i+i+1:end+1) = event(2*i+i:end); % move rows down

event(2\*i+i) = Nevent(i); % insert response mark

end

%% save event in case of overwrite

save event\_merge.mat event

%% if the EEG data was currently loaded, simply replace the EEG.event with event, then operate as normal

EEG.event = event;

%replace ERPLAB event if you are using ERPLAB plugin

ERP.EVENTLIST.eventinfo = event;</code></pre>



<p></p>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/merge-behavioral-data-into-eeg/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>How to place errorbars on a grouped bar graph in MATLAB</title>
		<link>http://jianch.github.io/how-to-place-errorbars-on-a-grouped-bar-graph-in-matlab/</link>
					<comments>http://jianch.github.io/how-to-place-errorbars-on-a-grouped-bar-graph-in-matlab/#respond</comments>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Mon, 12 Dec 2016 03:04:13 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Error bar]]></category>
		<category><![CDATA[Notes]]></category>
		<category><![CDATA[Tricks]]></category>
		<guid isPermaLink="false">http://jianch.github.io/?p=89</guid>

					<description><![CDATA[Well, although MATLAB is not great for plotting, a few tricks can make it drawing easier. We often use errorbar function to plot errorbar but it’s impossible for Matlab to put error bars on a grouped bar graph. Matlab simply gives a shit if you command it straightforwardly. For example: The error bars are located&#8230; <a class="more-link" href="http://jianch.github.io/how-to-place-errorbars-on-a-grouped-bar-graph-in-matlab/">Continue reading <span class="screen-reader-text">How to place errorbars on a grouped bar graph in MATLAB</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Well, although MATLAB is not great for plotting, a few tricks can make it drawing easier.</p>



<p>We often use errorbar function to plot errorbar but it’s impossible for Matlab to put error bars on a grouped bar graph. Matlab simply gives a shit if you command it straightforwardly. For example:</p>



<figure class="wp-block-image size-large"><img loading="lazy" width="1005" height="811" src="http://jianch.github.io/wp-content/uploads/2021/02/WX20161212-171137.png" alt="" class="wp-image-90" srcset="http://jianch.github.io/wp-content/uploads/2021/02/WX20161212-171137.png 1005w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161212-171137-300x242.png 300w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161212-171137-768x620.png 768w, http://jianch.github.io/wp-content/uploads/2021/02/WX20161212-171137-850x686.png 850w" sizes="(max-width: 1005px) 100vw, 1005px" /></figure>



<p>The error bars are located in the center of each grouped bars, rather than the individual bar in each grouped bars.</p>



<p>Here is the note I made, the main code is from the link below:</p>



<p><a href="https://au.mathworks.com/matlabcentral/answers/102220-how-do-i-place-errorbars-on-my-grouped-bar-graph-using-function-errorbar-in-matlab-7-13-r2011b">https://au.mathworks.com/matlabcentral/answers/102220-how-do-i-place-errorbars-on-my-grouped-bar-graph-using-function-errorbar-in-matlab-7-13-r2011b</a></p>



<figure class="wp-block-image size-large"><img loading="lazy" width="1024" height="732" src="http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy-1024x732.jpg" alt="" class="wp-image-92" srcset="http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy-1024x732.jpg 1024w, http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy-300x214.jpg 300w, http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy-768x549.jpg 768w, http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy-850x607.jpg 850w, http://jianch.github.io/wp-content/uploads/2021/02/GG01Dec16a_rt_visual-field-copy.jpg 1188w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption>MATLAB Handle Graphics</figcaption></figure>



<p></p>



<p>Code example:</p>



<pre class="wp-block-code"><code>Data set:

mm: 4 by 6 matrix; ci: 4 by 6 matrix



mm =

&#91;0.2081 0.3440 0.3470 0.7018 0.8422 0.9874
0.2628 0.3043 0.4032 0.7287 0.8462 1.1055
0.2928 0.3139 0.4176 0.7241 0.8319 1.0483
0.2287 0.2538 0.3665 0.4895 0.8999 1.1309]

ci =

&#91;0.0176 0.0473 0.0443 0.0724 0.0855 0.0931
0.0312 0.0201 0.0407 0.0621 0.0604 0.0874
0.0291 0.0330 0.0403 0.0756 0.0732 0.0847
0.0228 0.0150 0.0264 0.0598 0.0922 0.0861]</code></pre>



<pre class="wp-block-code"><code>figure;
h = bar(mm');
set(h,'BarWidth',0.8);    % The bars will now touch each other
set(gca,'fontsize', 18);
set(get(gca,'YLabel'),'String','RT in Seconds')
lh = legend('Valid Left', 'Invalid left', 'Valid right', 'Invalid right');
set(lh,'Location','BestOutside','Orientation','horizontal')
hold on;

% Aligning errorbar to individual bar within groups
% Based on barweb.m by Bolu Ajiboye from MATLAB File Exchange
groupwidth = min(0.8, 4/(4+1.5));
for i = 1:4
x = (1:6) - groupwidth/2 + (2*i-1) * groupwidth / (2*4);
errorbar(x,mm(i,:),ci(i,:),'k', 'linestyle', 'none');
end</code></pre>
]]></content:encoded>
					
					<wfw:commentRss>http://jianch.github.io/how-to-place-errorbars-on-a-grouped-bar-graph-in-matlab/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
