<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>saturn &#8211; Dr. Jian Chen</title>
	<atom:link href="http://localhost/jianchen/author/saturn/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.jianchen.info</link>
	<description>陈建博士</description>
	<lastBuildDate>Wed, 29 Jun 2022 01:36:41 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.0</generator>

<image>
	<url>https://www.jianchen.info/wp-content/uploads/2022/06/cropped-Frued_Cartoon-32x32.png</url>
	<title>saturn &#8211; Dr. Jian Chen</title>
	<link>https://www.jianchen.info</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Dot Enumeration Task with jsPsych</title>
		<link>https://www.jianchen.info/2022/06/29/dot-enumeration-task-with-jspsych/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Wed, 29 Jun 2022 01:36:40 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[dotEnumeration]]></category>
		<category><![CDATA[github]]></category>
		<category><![CDATA[jsPsych]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=163</guid>

					<description><![CDATA[This task was coded with jsPsych v7.2. The source code and materials are available in my GitHub Dr Jian Chen https://github.com/jianch/dotEnumerationTask]]></description>
										<content:encoded><![CDATA[
<p></p>



<blockquote class="wp-block-quote"><p>This task was coded with jsPsych v7.2. The source code and materials are available in my GitHub </p><cite>Dr Jian Chen</cite></blockquote>



<p><a href="https://github.com/jianch/dotEnumerationTask">https://github.com/jianch/dotEnumerationTask</a></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Install Process in SPSS in MacOS Big Sur</title>
		<link>https://www.jianchen.info/2021/09/09/install-process-in-spss-in-macos-big-sur/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 09 Sep 2021 00:14:18 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[MacOS]]></category>
		<category><![CDATA[SPSS]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=131</guid>

					<description><![CDATA[I don’t like SPSS, but due to the need of teaching, I have to install SPSS in my own computer. When I tried to install Hayes’ Process add-ons in the SPSS, it always said it has no permission to do so. This is quite annoying. So the reason is that SPSS has no full disk ... <a title="Install Process in SPSS in MacOS Big Sur" class="read-more" href="https://www.jianchen.info/2021/09/09/install-process-in-spss-in-macos-big-sur/" aria-label="More on Install Process in SPSS in MacOS Big Sur">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>I don’t like SPSS, but due to the need of teaching, I have to install SPSS in my own computer.</p>



<p>When I tried to install Hayes’ Process add-ons in the SPSS, it always said it has no permission to do so. This is quite annoying.</p>



<p>So the reason is that SPSS has no full disk access to the targeted folder. Why? I already gave full access to it.</p>



<p>It turns out that there are a few things changed since MacOS Catalina, the direct result is that SPSS lost full disk access to many folders in Mac. You have to give a quite specific permission to SPSS.</p>



<p>The video below is the solution for this issue.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe title="How to Fix SPSS File Access issue after Mac Catalina update - new apple update" width="1100" height="619" src="https://www.youtube.com/embed/gmBn5SneX2I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></figure>



<p>After you fix the full disk access issue, then you will be able to install Process.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Digit Span Task in Qualtrics</title>
		<link>https://www.jianchen.info/2021/07/23/digit-span-task-in-qualtrics/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Fri, 23 Jul 2021 00:13:10 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[digit span task]]></category>
		<category><![CDATA[Qualtrics]]></category>
		<category><![CDATA[working memory]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=129</guid>

					<description><![CDATA[The pandemic makes online psychological experiments very needed in the past few months. One commonly used tool is Qualtrics. I don’t have too many complains about Qualtrics except for its ability to DIY cognitive tasks. I don’t see why they don’t include this feature as many of us already bought their most expensive plans. Nevertheless, ... <a title="Digit Span Task in Qualtrics" class="read-more" href="https://www.jianchen.info/2021/07/23/digit-span-task-in-qualtrics/" aria-label="More on Digit Span Task in Qualtrics">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>The pandemic makes online psychological experiments very needed in the past few months. One commonly used tool is Qualtrics.</p>



<p>I don’t have too many complains about Qualtrics except for its ability to DIY cognitive tasks. I don’t see why they don’t include this feature as many of us already bought their most expensive plans.</p>



<p>Nevertheless, there are alternative ways to run cognitive tasks with Qualtrics surveys. One way is to use Inquisit Web. But we all know that participants really don’t like to download and install any software in their personal devices, so we see many participants drop out the study due to this reason. May I say that we are lucky enough if we can have 50% participants complete the Inquisit Web-based cognitive tasks? Other ways to run online cognitive tasks include the PsychoPy and PsyToolkit, but I won’t talk about them in this article.</p>



<p>The ideal way to run cognitive tasks along with Qualtrics survey, of course, is to run cognitive tasks within Qualtrics.</p>



<p>Here is an example of running digit span task within Qualtrics. I need to explicitly state that, the code is not from me. I developed my own working version with the help from Dr Becky Gilbert (@BeckyAGilbert). You can find her at&nbsp;<a href="https://www.mrc-cbu.cam.ac.uk/people/becky.gilbert/">https://www.mrc-cbu.cam.ac.uk/people/becky.gilbert/</a>. And of course, if you do use the task and the code, it would be appreciated either a citation or acknowledgment. You should also cite the QRTE paper:&nbsp;<a rel="noreferrer noopener" href="https://link.springer.com/article/10.3758/s13428-014-0530-7" target="_blank">https://link.springer.com/article/10.3758/s13428-014-0530-7</a>&nbsp;.</p>



<p><strong>You can find&nbsp;Dr Becky Gilbert’s Digit Span Task at this link:&nbsp;<a href="https://uclpsych.eu.qualtrics.com/jfe/form/SV_55fg8t1Q3MGO8U5?Q_JFE=qdg">https://uclpsych.eu.qualtrics.com/jfe/form/SV_55fg8t1Q3MGO8U5?Q_JFE=qdg</a></strong>. The first half of the task works fine but the second half does not work. This is because&nbsp;QRTE JavaScript library is no longer supported.<br><br>Here is the QSF file that exported from Qualtrics.</p>



<p><a href="https://jianch.github.io/wp-content/uploads/2021/07/digit_span_demo.qsf_.zip">digit_span_demo.qsf_</a><a href="https://jianch.github.io/wp-content/uploads/2021/07/digit_span_demo.qsf_.zip">Download</a></p>



<p>To get it to work properly, you may need to go into the survey settings and remove any default styling that your institution applies to&nbsp;Qualtrics&nbsp;surveys. You’ll want to use the plain/basic styling option for this task. You can also look at the QRTE documentation for more information about how it works:&nbsp;<a rel="noreferrer noopener" href="http://www.qrtengine.com/" target="_blank">http://www.qrtengine.com/</a></p>



<p>In my case, I have to put the forward and backward parts separately into two Qualtrics surveys, otherwise it just won’t work. I would recommend you do the same thing.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Embed Youtube video in the Inquisit Web in Windows and Mac</title>
		<link>https://www.jianchen.info/2020/04/06/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Mon, 06 Apr 2020 00:11:19 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[Inquisit]]></category>
		<category><![CDATA[Youtube]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=127</guid>

					<description><![CDATA[It is necessary to use online video source when you want to present long video clips in an Inquisit Web script. This is because the Inquisit Web server only provides 60MB space for you, although we all pay them a big amount of money, such a small storage space can only host a few short ... <a title="Embed Youtube video in the Inquisit Web in Windows and Mac" class="read-more" href="https://www.jianchen.info/2020/04/06/embed-youtube-video-in-the-inquisit-web-in-windows-and-mac/" aria-label="More on Embed Youtube video in the Inquisit Web in Windows and Mac">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>It is necessary to use online video source when you want to present long video clips in an Inquisit Web script. This is because the Inquisit Web server only provides 60MB space for you, although we all pay them a big amount of money, such a small storage space can only host a few short and low resolution videos.</p>



<p>Thankfully, Inquisit is able to handle html page, which is great. Theoretically, you only need to embed an iframe element in the Inquisit script and it should be able to fetch video from Youtube or other sources.&nbsp;</p>



<p>You will need an embeded video link like this, it can be obtained from Youtube Share.</p>



<figure class="wp-block-embed"><div class="wp-block-embed__wrapper">
https://youtube.com/watch?v=LW2oswdWs4Q%3Frel%3D0%26controls%3D0%26showinfo%3D0%26autoplay%3D1
</div></figure>



<p>Then you need to add a html element in the Inquisit, it looks like this:</p>



<pre class="wp-block-code"><code>&lt;html demo&gt;
/ items = videos
/ select = 1
/ size = (100%,100%)
/ showborders = false
/ showscrollbars = false
&lt;/html&gt;</code></pre>



<p>However, this method only works fine in MacOS, it shows blank when you run the Inquisit script in a Windows computer. This is a known issue and there is no official solution yet.&nbsp;</p>



<p>————————————<br><strong>Q:&nbsp;</strong><br>I’ve used html to embed videos in my experiment that I have uploaded on youtube.<br>The embedded links when directly entered into the browser (firefox, internet explorer) work fine. Within inquisit it also works fine on a mac, but I’m encountering problems on windows (on some windows laptops it only shows a black box rather than the video). Also, there seem to be a difference between windows 10 and windows 7, since it did work on someone’s windows 7 laptop.<br>Is there a difference for html on mac vs windows?</p>



<p><strong>A:&nbsp;</strong><br>Yes, there are differences between OSX and Windows in how HTML is handled. Essentially, under Windows, Inquisit embeds Internet Explorer (because it is reliably available on all Windows systems) to render HTML. When run in embedded mode like this, however, Internet Explorer enforces various restrictions that may keep embedded / interactive content such as videos from working. The way a given system is set up and what security settings the user or organization applied to Internet Explorer installations also play a role under some circumstances, i.e. content may work on one system, but not on a different one with different settings / restrictions applied.<br><strong>I’m afraid there isn’t a really good or universal solution here.&nbsp;</strong>Ideally, you’d not embed Youtube videos, but instead you’d use standard \&lt;video&gt; elements in Inquisit to play the videos. For online use, you can set the \&lt;video&gt; elements’ /stream attributes to true, that way the won’t have to be downloaded in full before the experiment launches, they’ll be streamed instead at runtime.<br><em><a href="https://www.millisecond.com/forums/Topic25195.aspx" target="_blank" rel="noreferrer noopener">https://www.millisecond.com/forums/Topic25195.aspx</a></em><br>————————————</p>



<p>A possible workaround is to right click on the black screen and then choose a different encoding, such as “Western European (Windows)”. Then the online video will be loaded and displayed on the Inquisit. There seems to be an encoding issue in running Inquisit script in Windows.</p>



<figure class="wp-block-image"><img src="https://jianch.github.io/wp-content/uploads/2021/02/rightClick-1024x473.png" alt="" class="wp-image-143"/></figure>



<p>So, is there a “<strong>really good or universal solution</strong>” for this problem? The answer is&nbsp;<strong>YES.</strong></p>



<p>My idea here is, if the Inquisit is able to handle html element, can we directly insert a DIY html page rather than let the Inquisit build a html page?&nbsp;</p>



<p>Here, I created a html page with some help from W3Schools and other resources:_(<a rel="noreferrer noopener" href="https://benmarshall.me/responsive-iframes/" target="_blank">https://benmarshall.me/responsive-iframes/</a>;&nbsp;<a rel="noreferrer noopener" href="https://www.w3schools.com/tags/att_body_bgcolor.asp" target="_blank">https://www.w3schools.com/tags/att_body_bgcolor.asp</a>;&nbsp;<a rel="noreferrer noopener" href="https://stackoverflow.com/questions/15844500/shrink-a-youtube-video-to-responsive-width" target="_blank">https://stackoverflow.com/questions/15844500/shrink-a-youtube-video-to-responsive-width</a>)_</p>



<pre class="wp-block-code"><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
.iframe-container {
 overflow: hidden;
 padding-top: 56.25%;
 position: relative;
}
.iframe-container iframe {
  border: 0;
  height: 100%;
  left: 0;
  position: absolute;
  top: 0;
  width: 100%;
}
/* 4x3 Aspect Ratio */
.iframe-container-4x3 {
 padding-top: 75%;
}
&lt;/style&gt;
&lt;/head&gt;

&lt;body bgcolor="black"&gt;

&lt;div class="iframe-container"&gt;
 &lt;iframe src="<a href="https://www.youtube.com/embed/LW2oswdWs4Q?~~start=10&amp;rel=0&amp;controls=0&amp;showinfo=0&amp;autoplay=1">https://www.youtube.com/embed/LW2oswdWs4Q?~~start=10&amp;rel=0&amp;controls=0&amp;showinfo=0&amp;autoplay=1</a>" allowfullscreen=1&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</code></pre>



<p>You can create an empty html document and the copy these code into your document, or you can download a html file from&nbsp;<a href="https://jianchen.info/files/inquisitOnlineVideo/demo.html">here</a>.</p>



<p>When you have the DIY html file, let’s see an Inquisit&nbsp;<a href="https://jianchen.info/files/inquisitOnlineVideo/demo.iqx">demo</a>. This demo is extracted and modified from somewhere in the Inquisit forum. Nevertheless, it’s quite easy to make your own one.</p>



<pre class="wp-block-code"><code>&lt;html videoDemo&gt;
/ items = ("demo.html")
/ size = (100%, 100%) 
/ erase = true(255, 255, 255)
/ showborders = false
/ showscrollbars = false
&lt;/html&gt;

&lt;trial videoDemo&gt;
/ stimulustimes = &#91;500=videoDemo]
/ trialduration = 50000
/ ontrialend= &#91;trial.videoDemo.resetstimulusframes();]
&lt;/trial&gt;

&lt;block videoDemo&gt;
/ trials = &#91;1=videoDemo]
&lt;/block&gt;

&lt;expt&gt;
/ blocks = &#91;1=videoDemo]
&lt;/expt&gt;</code></pre>



<p>And boom! It works! Now we can display Youtube video in Inquisit in a damn Windows pc!</p>



<figure class="wp-block-image"><img src="https://jianch.github.io/wp-content/uploads/2021/02/htmlScreenshot-1024x583.png" alt="" class="wp-image-144"/></figure>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Change the space between paragraphs in Qualtrics</title>
		<link>https://www.jianchen.info/2019/11/12/change-the-space-between-paragraphs-in-qualtrics/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Tue, 12 Nov 2019 00:10:12 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[Qualtrics]]></category>
		<category><![CDATA[space]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=125</guid>

					<description><![CDATA[The space between paragraphs in the Qualtrics might be too large, how to reduce the space?&#160; One solution is to add JS code in the “Question JavaScript” to change the padding size or margin size before and after one paragraph. Note, Qualtrics appears to set up some kind of minimum space between paragraphs so even ... <a title="Change the space between paragraphs in Qualtrics" class="read-more" href="https://www.jianchen.info/2019/11/12/change-the-space-between-paragraphs-in-qualtrics/" aria-label="More on Change the space between paragraphs in Qualtrics">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>The space between paragraphs in the Qualtrics might be too large, how to reduce the space?&nbsp;</p>



<p>One solution is to add JS code in the “Question JavaScript” to change the padding size or margin size before and after one paragraph.</p>



<pre class="wp-block-code"><code>jQuery("#"+this.questionId).find('.QuestionText:first').css("padding-bottom", "0px");
jQuery("#"+this.questionId).find('.QuestionText:first').css("padding-top", "0px");</code></pre>



<p>Note, Qualtrics appears to set up some kind of minimum space between paragraphs so even if you change the padding value to 0, the space might still be large.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Plot Heatmap from Eye Tracking Data</title>
		<link>https://www.jianchen.info/2017/09/07/plot-heatmap-from-eye-tracking-data/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 07 Sep 2017 00:09:14 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[eyetracker]]></category>
		<category><![CDATA[heatmap]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=123</guid>

					<description><![CDATA[There are many ways/toolbox to plot a heat map from eye tracking data. but I prefer DIY Here is the source code:]]></description>
										<content:encoded><![CDATA[
<p>There are many ways/toolbox to plot a heat map from eye tracking data. but I prefer DIY</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Here is the source code:</p>



<pre class="wp-block-code"><code>%% Data Structure Explanation
% Explanation from Eyelink Programers Guide 3.0

%--------------------------------------------------------
% dataEyelink =
%
%       samples: &#91;401341x4 double]()
%     fixations: &#91;3965x6 double]()
%      saccades: &#91;3964x9 double]()
%        blinks: &#91;1720x3 double]()
%      triggers: &#91;1080x3 double]()

%--------------------------------------------------------
% dataEyelink.samples
%       column1         column2         column3     column4
%       timepoint       x               y           pupil size
%--------------------------------------------------------
% dataEyelink.fixations
%       column1         column2         column3     column4 column5 column6
%       tmepoint_start  timepoint_end   duration    x       y       avg pupil size
%--------------------------------------------------------
% dataEyelink.saccades
%       column1         column2         column3     column4 column5 column6
%       tmepoint_start  timepoint_end   duration    x_from  y_from  x_to
%       column7         column8         column9
%       y_to            amplitude       peak velocity
%                       in degrees      degr/sec
%--------------------------------------------------------
% dataEyelink.blinks
%       column1         column2         column3
%       tmepoint_start  timepoint_end   duration
%--------------------------------------------------------
% dataEyelink.triggers
%       column1         column2         column3
%       tmepoint        1=SYNCON        Trial(I also wrote trial number)
%                       0=SYNCOFF
%--------------------------------------------------------



clear;
load xxxxEyelink.mat

%% variables
gaussSigma = 0.05;
posX = round(dataEyelink.fixations(:,4));
posY = round(dataEyelink.fixations(:,5));
gazeDuration = dataEyelink.fixations(:,3) / max(dataEyelink.fixations(:,3)); % rescale to 0-1

%% generating data for heatmap
gazedata = &#91;posX/1024, posY/768](); % rescale to 0-1
gazedata = gazedata((gazedata(:, 1))\&gt;0, :); % remove possible negative value...

%% make gaussians
figure;
&#91;X,Y]() = meshgrid(0:0.001:1, 0:0.001:1);
z = zeros(size(X,1),size(X,2));

for i = 1:length(gazedata)
z = z + gazeDuration(i) * exp(-( ((X - gazedata(i,1)).^2 ./ (2*gaussSigma^2)) + ((Y - gazedata(i,2)).^2 ./ (2*gaussSigma^2)) ) );
end

mesh(X,Y,z); % plot the heatmap
colorbar;
caxis(&#91;0,300]());
view(0,90);


print('heatmap', '-dtiff','-r300');</code></pre>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Corrected Vision May Still Bias the Gabor Patch Detection</title>
		<link>https://www.jianchen.info/2017/07/12/corrected-vision-may-still-bias-the-gabor-patch-detection/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Wed, 12 Jul 2017 00:07:57 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[Astigmatism]]></category>
		<category><![CDATA[corrected vision]]></category>
		<category><![CDATA[Gabor patch]]></category>
		<category><![CDATA[vision]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=121</guid>

					<description><![CDATA[As said in the title, participants with corrected vision (wearing glass or contact lens) may still have distorted vision due to whatever reason, especially in Gabor patch detection task. This question comes from my personal experience, I always feel that the vertical Gabor patch is brighter than horizontal ones, and is easier to detect. Below ... <a title="Corrected Vision May Still Bias the Gabor Patch Detection" class="read-more" href="https://www.jianchen.info/2017/07/12/corrected-vision-may-still-bias-the-gabor-patch-detection/" aria-label="More on Corrected Vision May Still Bias the Gabor Patch Detection">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>As said in the title, participants with corrected vision (wearing glass or contact lens) may still have distorted vision due to whatever reason, especially in Gabor patch detection task.</p>



<p>This question comes from my personal experience, I always feel that the vertical Gabor patch is brighter than horizontal ones, and is easier to detect. Below is my test data from 800 trials of a detection task (error bar represents 1 SE).</p>



<p>I gradually increased the Gabor patch contrast from 0 to 100%, I pressed a button when I saw something on the screen (grey background).</p>



<p><img src="https://jianchen.info/images/gabor/CJ04May17a.png" alt=""><br>Session 1</p>



<p><img src="https://jianchen.info/images/gabor/CJ04May17b.png" alt=""><br>Session 2</p>



<p>I am quite curious about why I feel so differently for these two kinds of Gabor patch, am I special? Do I have something wrong with my eye?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>So I asked two colleague (they don’t wear glass regularly) to do the same task, and it turns out no difference between vertical and horizontal Gabor patches.</p>



<p><img src="https://jianchen.info/images/gabor/CV04May17a.png" alt=""><br>Colleague 1</p>



<p><img src="https://jianchen.info/images/gabor/JN06May17a.png" alt=""><br>Colleague 2</p>



<p>Then what’s going on underneath?</p>



<p>I doubt that although I have a glass to correct my vision, I may not have astigmatism corrected. So I asked a friend, who also wear a glass, to do the same task. As you can see, he has the same problem.</p>



<p><img src="https://jianchen.info/images/gabor/ZL28Jun17a.png" alt=""><br>Friend 1</p>



<p>Insofar, my guess of the explanation for such results is astigmatism. Me and my friend both have inappropriately corrected vision, which leads to the difference in detection task.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>You may have a quick test to see if you have astigmatism. All the bars in below should have similar brightness.</p>



<p><img src="https://jianchen.info/images/gabor/EYE_astigmatism.gif" alt=""><br>Astigmatism Test</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>What’s the hint from this observation? Well, it may remind us to make sure that participants who claim a correctly corrected vision should be re-checked, especially in those vision tasks relying on Gabor patch detection.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Get real-time gaze positions from the eye tracker</title>
		<link>https://www.jianchen.info/2017/03/30/get-real-time-gaze-positions-from-the-eye-tracker/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 30 Mar 2017 00:06:55 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[eyelink]]></category>
		<category><![CDATA[eyetracker]]></category>
		<category><![CDATA[gaze]]></category>
		<category><![CDATA[matlab]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=119</guid>

					<description><![CDATA[If I want to make sure the participants are fixating on the cross before starting each trial, how should I do? With the help of Eyetracker (Eyelink in this case), we can get the realtime gaze position and therefore we can monitor the gaze position to make sure it falls into a certain area before ... <a title="Get real-time gaze positions from the eye tracker" class="read-more" href="https://www.jianchen.info/2017/03/30/get-real-time-gaze-positions-from-the-eye-tracker/" aria-label="More on Get real-time gaze positions from the eye tracker">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>If I want to make sure the participants are fixating on the cross before starting each trial, how should I do?</p>



<p>With the help of Eyetracker (Eyelink in this case), we can get the realtime gaze position and therefore we can monitor the gaze position to make sure it falls into a certain area before the participants start the trial.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<pre class="wp-block-code"><code>% make sure participants are fixating on the central
while isEyeLink  % if Eyelink was connected
status = Eyelink('newfloatsampleavailable');  % check to see if everything is fine in Eyelink
if status /=0; % if all is fine
evt = Eyelink('newestfloatsample'); % get the newest float sample from Eyelink
realtime.x = evt.gx(1);  % get the x axis of gaze
realtime.y = evt.gy(1);  % get the y axis of gaze
end;

if abs(realtime.x-512)\&lt;=50&amp;abs(realtime.y-384)\&lt;=50  % if the gaze falls into  a 50\*50 pixel square around the cross
break; % jump out the loop and start the trial
end
end</code></pre>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Keyboard Calibration Using the Damn Laser</title>
		<link>https://www.jianchen.info/2017/03/09/keyboard-calibration-using-the-damn-laser/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Thu, 09 Mar 2017 00:04:34 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[calibration]]></category>
		<category><![CDATA[keyboard]]></category>
		<category><![CDATA[laser]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=115</guid>

					<description><![CDATA[Almost no one care about how precise/reliable the keyboard pressing is, here, we built a damn cool equipment to examine this question. Broadly speaking, the time precision is good, mean variance is less than 0.2 ms. Environment: MATLAB with Linux (Ubuntu 14.04) Monitor: SONY Triniton at 120Hz Keyboard: Mechanic number board In short, the delay ... <a title="Keyboard Calibration Using the Damn Laser" class="read-more" href="https://www.jianchen.info/2017/03/09/keyboard-calibration-using-the-damn-laser/" aria-label="More on Keyboard Calibration Using the Damn Laser">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>Almost no one care about how precise/reliable the keyboard pressing is, here, we built a damn cool equipment to examine this question. Broadly speaking, the time precision is good, mean variance is less than 0.2 ms.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Environment: MATLAB with Linux (Ubuntu 14.04)</p>



<p>Monitor: SONY Triniton at 120Hz</p>



<p>Keyboard: Mechanic number board</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<figure class="wp-block-image"><img src="https://jianchen.info/images/laser/illustration.png" alt=""/></figure>



<p></p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<figure class="wp-block-image size-large"><img loading="lazy" width="1024" height="481" src="https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-1024x481.jpg" alt="" class="wp-image-116" srcset="https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-1024x481.jpg 1024w, https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-300x141.jpg 300w, https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-768x361.jpg 768w, https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-1536x722.jpg 1536w, https://www.jianchen.info/wp-content/uploads/2022/06/Figure.2.3_figure-2048x962.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>In short, the delay is about 12 ms and the variation is quite small. </p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>EEG Processing Steps (based on EEGLAB and ERPLAB)</title>
		<link>https://www.jianchen.info/2017/01/18/eeg-processing-steps-based-on-eeglab-and-erplab/</link>
		
		<dc:creator><![CDATA[saturn]]></dc:creator>
		<pubDate>Wed, 18 Jan 2017 00:01:26 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<category><![CDATA[eeg]]></category>
		<category><![CDATA[eeglab]]></category>
		<category><![CDATA[matlab]]></category>
		<guid isPermaLink="false">https://www.jianchen.info/?p=113</guid>

					<description><![CDATA[Here are the basic steps I used to process BioSemi EEG data. 1 load raw data 2 channel location 3 re-reference 4 filtering&#160;(sometimes the data is very huge and therefore the filtering will be very slow, so we can do the filtering later) 5 Create EEG event list (ERPLAB from now on) 6 Assign bins ... <a title="EEG Processing Steps (based on EEGLAB and ERPLAB)" class="read-more" href="https://www.jianchen.info/2017/01/18/eeg-processing-steps-based-on-eeglab-and-erplab/" aria-label="More on EEG Processing Steps (based on EEGLAB and ERPLAB)">Read more</a>]]></description>
										<content:encoded><![CDATA[
<p>Here are the basic steps I used to process BioSemi EEG data.</p>



<p><strong>1 load raw data</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/809C496A-8039-4D2E-9B13-70B6B7721A10.png" alt=""/></figure>



<p><strong>2 channel location</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/030CC12C-9735-43A2-AD38-53EBF6240A46.png" alt=""/></figure>



<p><strong>3 re-reference</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/3135436B-A01A-4FED-9B9E-E426C7FA3CB4.png" alt=""/></figure>



<p><strong>4 filtering</strong>&nbsp;(sometimes the data is very huge and therefore the filtering will be very slow, so we can do the filtering later)</p>



<p><strong>5 Create EEG event list (ERPLAB from now on)</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/F18A0B75-3355-4EBE-B7CA-E62720858371.png" alt=""/></figure>



<p><strong>6 Assign bins (how to operate on bins? see&nbsp;<a href="https://github.com/lucklab/erplab/wiki/Assigning-Events-to-Bins-with-BINLISTER:-Tutorial" target="_blank" rel="noreferrer noopener">here</a>)</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/43ABAD39-3E41-4BA7-9607-34EA61FE7D74.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/8A8EC18C-86E4-4AB8-8242-1E6D23716321.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/4AC21F49-0134-4FE3-83E1-12B663C404DF.png" alt=""/></figure>



<p><strong>7 extract bin-based epoch</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/B1B02461-7F46-4AB1-8371-AF769E9F3E26.png" alt=""/></figure>



<p><strong>8 artifact rejection</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/40120CE4-073E-4CEF-A35D-98DDB2DDD429.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/9234B231-7612-41D2-9349-5682C3027FD8.png" alt=""/></figure>



<p><strong>-&gt; choose channel around the eye. -&gt; update and then reject</strong></p>



<p><strong>9 compute average erps</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/289AC38D-87CF-4234-9B90-D5C01D010D9A.png" alt=""/></figure>



<p><strong>10 filter if you don’t do it in step 4</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/CD5AD34D-B8EC-4825-B5A8-90EA5C31611D.png" alt=""/></figure>



<p><strong>11 bin operations if necessary&nbsp;</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/1E8272A4-A250-4471-945D-2D764439AE61.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/3FA48AA1-9212-4A49-A8B4-73CEFFB13B38.png" alt=""/></figure>



<p><strong>12 Plot ERPs</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/AB81310E-9BC5-422B-95FD-B3718883494D.png" alt=""/></figure>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/F1F2CFBD-3018-4E77-B960-35EEFB103226.png" alt=""/></figure>



<p><strong>13 Save figures if you want</strong></p>



<figure class="wp-block-image"><img src="https://jianchen.info/images/eegsteps/14A0D60A-40B2-4697-9F30-84312AFCEEBF.png" alt=""/></figure>



<p>Remember, these are basic steps to illustrate how to process the EEG data using EEGLAB and ERPLAB toolbox, you’re suggested not to strictly follow these steps.</p>



<p><a href="https://jianch.github.io/author/saturn/"></a></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
